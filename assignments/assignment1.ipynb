{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO NOT EDIT IF INSIDE `annadl_f19` folder** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handin in Peergrade**: *Monday*, October 14, 2019, 23:59<br>\n",
    "**Peergrading deadline**: *Friday*, October 18, 2019, 23:59<br>\n",
    "**Peergrading feedback deadline**: *Sunday*, October 20, 2019, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Feedback**](http://ulfaslak.com/vent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.2.2**: Create a neural network with `sizes=[2, 1]`. This is actually just our most basic Perceptron model with two inputs and a single output. Generate 1e5 training datapoints and 1e4 testing datapoints using the `generate_X_linear` function, and fit the neural network to this data. Once you've fitted the network, make two plots of your test datapoints, one where points are colored by predictions and another where points are colored by their true labels. Can you say something about the points that are being mislabeled?\n",
    ">\n",
    ">*Hint*: You'll probably find some of Nielsen's examples from [Chapter 1](http://neuralnetworksanddeeplearning.com/chap1.html) in his book useful. Also, note that predictions are values between 0 and 1, not hard 0s and 1s, due to the sigmoid activation function. You can deal with this however you like when you color the points by prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def step(z, threshold=0.5):\n",
    "    if z > threshold:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# Feed forward neural network class\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        \n",
    "        # Q: Print these out, explain their contents. You can instantiate a network by\n",
    "        # doing `net = Network([2, 3, 1])`, and then printing `net.biases`.\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        \n",
    "        # Q: What is `a`? How many iterations will this loop run? For a `sizes=[2, 3, 1]`\n",
    "        # network, what is the shape of `a` at each iteration?\n",
    "        for b, w in zip(self.biases, self.weights): # \n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None, silent=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        \n",
    "        # Ulf: For now we just treat this function as a black box.\n",
    "        \n",
    "        n = len(training_data)\n",
    "        if test_data:\n",
    "            n_test = len(test_data)\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)\n",
    "            ]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            \n",
    "            if not silent:\n",
    "                if test_data:\n",
    "                    print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test))\n",
    "                else:\n",
    "                    print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        \n",
    "        # Ulf: For now we just treat this function as a black box.\n",
    "\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        self.weights = [\n",
    "            w - eta / len(mini_batch) * nw\n",
    "            for w, nw in zip(self.weights, nabla_w)\n",
    "        ]\n",
    "        self.biases = [\n",
    "            b - eta / len(mini_batch) * nb\n",
    "            for b, nb in zip(self.biases, nabla_b)\n",
    "        ]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        \n",
    "        # Ulf: For now we just treat this function as a black box.\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book. Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on. It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return nabla_b, nabla_w\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        if self.sizes[-1] == 1:\n",
    "            test_results = [\n",
    "                (step(self.feedforward(x)), y)\n",
    "                for x, y in test_data\n",
    "            ]\n",
    "        else:\n",
    "            test_results = [\n",
    "                (np.argmax(self.feedforward(x)), y)\n",
    "                for x, y in test_data\n",
    "            ]\n",
    "        return sum(int(y_pred == y) for (y_pred, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return output_activations - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X_linear(N=200):\n",
    "    X = np.vstack([\n",
    "        np.random.normal([-2, -2], 1, size=(int(N/2), 2)),\n",
    "        np.random.normal([2, 2], 1, size=(int(N/2), 2))\n",
    "    ])\n",
    "\n",
    "    y = np.array([0] * int(N/2) + [1] * int(N/2)).reshape(-1, 1)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_X_linear(1e5)\n",
    "x_test, y_test = generate_X_linear(1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network([2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X, y):\n",
    "    \"\"\"Format the dataset X and y so it fits with Nielsen's code.\"\"\"\n",
    "    return [\n",
    "        (X[i].reshape(-1, 1), y[i].reshape(-1, 1))  # our tuple (x, y)\n",
    "        for i in range(len(y))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cmap_in_range:\n",
    "    \"\"\"Create map to range of colors inside given domain.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> cmap = cmap_in_range([0, 1])\n",
    "    >>> cmap(0.1)\n",
    "    (0.30392156862745101, 0.30315267411304353, 0.98816547208125938, 1.0)\n",
    "    \"\"\"\n",
    "    def __init__(self, cmap_domain, cmap_range=[0, 1], cmap_style='rainbow'):\n",
    "        self.cmap_domain = cmap_domain\n",
    "        self.cmap_range = cmap_range\n",
    "        self.m = interp1d(cmap_domain, cmap_range)\n",
    "        self.cmap = plt.get_cmap(cmap_style)\n",
    "        \n",
    "    def __call__(self, value):\n",
    "        if not self.cmap_domain[0] <= value <= self.cmap_domain[1]:\n",
    "            raise Exception(\"Value must be inside cmap_domain.\")\n",
    "        return self.cmap(self.m(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_linear = prepare_data(x_train ,y_train)\n",
    "test_set_linear = prepare_data(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.SGD(train_set_linear, epochs=5, mini_batch_size=100, eta=15.0, test_data=test_set_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = np.array([network.feedforward(xy[0]) for xy in test_set_linear])\n",
    "preds = np.array([x[0] for x in preds]).flatten()\n",
    "\n",
    "truths = np.array([xy[1][0] for xy in test_set_linear])\n",
    "         \n",
    "coords = np.array([xy[0] for xy in test_set_linear])\n",
    "Xs = [c[0] for c in coords]\n",
    "Ys = [c[1] for c in coords]\n",
    "\n",
    "cmap = cmap_in_range([0,1])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x_test[:,0], x_test[:,1], c=[cmap(t) for t in y_test.flatten()])\n",
    "plt.title(\"Ground truth\", fontsize=20)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x_test[:,0], x_test[:,1], c=[cmap(pred) for pred in preds])\n",
    "plt.title(\"Nonlinear predictions\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only a few points that are mislabeled. These misclassified points are basically outliers of each group - positioned far from the center of the rest of its group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.2.3**: Now we want to do the same thing as in Ex. 1.2.2, but with the data generated with the `generate_X_nonlinear` function.\n",
    ">\n",
    ">*Hint*: Think back to the slides to figure out what `sizes` should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X_nonlinear(N=200, R=5):\n",
    "    X_inner = np.random.normal([0, 0], 1, size=(int(N/2), 2))\n",
    "\n",
    "    X_outer = np.array([\n",
    "        [R*np.cos(theta), R*np.sin(theta)]\n",
    "        for theta in np.linspace(0, 2 * np.pi, int(N/2))\n",
    "    ]) + np.random.randn(int(N/2), 2)\n",
    "\n",
    "    X = np.vstack([X_inner, X_outer])\n",
    "    y = np.array([0] * int(N/2) + [1] * int(N/2)).reshape(-1, 1)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_nonlinear = Network([2, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training and testing data\n",
    "train_x_nonlin, train_y_nonlin = generate_X_nonlinear(1e5)\n",
    "test_x_nonlin, test_y_nonlin = generate_X_nonlinear(1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_nonlin = prepare_data(train_x_nonlin, train_y_nonlin)\n",
    "testing_set_nonlin = prepare_data(test_x_nonlin, test_y_nonlin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the neural network\n",
    "network_nonlinear.SGD(training_set_nonlin, epochs=5, mini_batch_size=100, eta=20.0, test_data=testing_set_nonlin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([network_nonlinear.feedforward(xy[0]) for xy in testing_set_nonlin])\n",
    "preds = np.array([x[0] for x in preds]).flatten()\n",
    "\n",
    "truths = np.array([xy[1][0] for xy in testing_set_nonlin])\n",
    "         \n",
    "coords = np.array([xy[0] for xy in testing_set_nonlin])\n",
    "Xs = [c[0] for c in coords]\n",
    "Ys = [c[1] for c in coords]\n",
    "\n",
    "cmap = cmap_in_range([0,1])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(test_x_nonlin[:,0], test_x_nonlin[:,1], c=[cmap(t) for t in test_y_nonlin.flatten()])\n",
    "plt.title(\"Ground truth\", fontsize=20)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(test_x_nonlin[:,0], test_x_nonlin[:,1], c=[cmap(pred) for pred in preds])\n",
    "plt.title(\"Nonlinear predictions\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points that are mislabeled are those that deviate from the group of points in the origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/abjer/tsds/master/material_exercises/week_3/2_3_1_net.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2.2.1**: The network above has a defined input and weights. If the true label for the datapoint `[4, 2]` is 1, what is the cost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "a = sigmoid(1 + 0.5*4 + 1*2)\n",
    "b = sigmoid(-1 + 2*4 + 10*2)\n",
    "c = sigmoid(2 + -5*4 + 0.3*2)\n",
    "d = sigmoid(-3 + 12*a + -8*b + 0.2*c)\n",
    "\n",
    "cost = (1 - d)**2\n",
    "print('The cost is %f' % cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2.2.2**: Knowing about backpropagation, we actually have everything we need to compute the gradients of the weights by hand. So go ahead and do that. Report your answer either as a diagram that includes the gradients (you can draw on my figure somehow and insert the resulting image), or just by writing what the gradient of each weight is.\n",
    ">\n",
    "> *Hint: When computing gradients with backprop, it can be a bit easier to think of the network as a computational graph. My computational graph looks like [this](https://github.com/abjer/tsds/blob/master/material_exercises/week_3/2_3_1_net_compgraph.png?raw=true).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION:\n",
    "\n",
    "![solution_image](https://camo.githubusercontent.com/0aedbec64a1fc349a9785fc76a49db0a8b618a7d/68747470733a2f2f7777772e65646e617372617463697065732e636f6d2f696d616765732f6e65745f636f6d7067726170685f66696c6c65642e706e67)\n",
    "\n",
    "In case the image is too small click [here](https://camo.githubusercontent.com/0aedbec64a1fc349a9785fc76a49db0a8b618a7d/68747470733a2f2f7777772e65646e617372617463697065732e636f6d2f696d616765732f6e65745f636f6d7067726170685f66696c6c65642e706e67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.1.3** Solve exercises 1.2.2 and 1.2.3 from week 1, but solve them using Keras.\n",
    "> Comment on differences in speed (and outcome if you observe any).\n",
    "You can for example use Tensorboard to inspect the training performance throught the training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION:\n",
    "\n",
    "### exercise 1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training and test data\n",
    "x_train_lin, y_train_lin = generate_X_linear(1e5)\n",
    "x_test_lin, y_test_lin = generate_X_linear(1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.2 model [2, 1]\n",
    "\n",
    "model_lin = Sequential() # sequential model\n",
    "\n",
    "model_lin.add(Dense(units=2, activation='sigmoid', input_shape=(2,))) # input shape (N, 2)\n",
    "model_lin.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model_lin.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lin.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lin.fit(x_train_lin, y_train_lin, epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lin.evaluate(x_test_lin, y_test_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exercise 1.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training and test data\n",
    "x_train_nonlin, y_train_nonlin = generate_X_nonlinear(1e5)\n",
    "x_test_nonlin, y_test_nonlin = generate_X_nonlinear(1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.2.3 model [2, 3, 1]\n",
    "\n",
    "model_nonlin = Sequential()\n",
    "\n",
    "model_nonlin.add(Dense(units=2, activation='sigmoid', input_shape=(2, )))\n",
    "model_nonlin.add(Dense(units=3, activation='sigmoid'))\n",
    "model_nonlin.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# optimizer\n",
    "sgd = optimizers.SGD(lr=1.5)\n",
    "\n",
    "model_nonlin.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model_nonlin.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nonlin.fit(x_train_nonlin, y_train_nonlin, epochs=15, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nonlin.evaluate(x_test_nonlin, y_test_nonlin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same architecture, there seems no difference in training speed or performance for exercise 1.2.2. However, there looks to be a differencei n both speed and performance for exercise 1.2.3. In fact, the Keras implementation trains slower and performs worse given the same hyperparameters. Even after adjusting for the hyperparameters, the Keras implementation still seems to lag behind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.2.1** To understand overfitting, we overfit some data! Let's fit a neural network to data that has no inherent patterns: *noise*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:59:04.927430Z",
     "start_time": "2019-09-23T20:59:04.739689Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate some data-points\n",
    "x = np.random.random(size=(50, 2))\n",
    "\n",
    "# And some random outcomes\n",
    "y = np.random.randint(0, 2, size=(50, ))\n",
    "\n",
    "# Plot it\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:41:33.645158Z",
     "start_time": "2019-09-23T20:41:33.333649Z"
    }
   },
   "source": [
    "> Now, create a network that attains a an accuracy score over 80% in predicting which points are class 0 and which are class 1.\n",
    ">\n",
    "> *Hint*: The bigger the network, the bigger the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "logdir = './logs'# + datetime.now().strftime(\"%Y%m%d-%H%M%S\") # log file name\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_overfit = Sequential()\n",
    "\n",
    "model_overfit.add(Dense(units=1024, activation='sigmoid', input_shape=(2,)))\n",
    "model_overfit.add(Dense(units=1024, activation='sigmoid'))\n",
    "model_overfit.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model_overfit.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The cell blow may take a couple of minutes but should overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_overfit.fit(x, y, epochs=5000, batch_size=100, verbose=1, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the formula for computing the size of the activation map resulting from a convolution. \n",
    "If you have a filter that is $F$ wide, your input image is $W_0$ wide, you are padding the edges by\n",
    "$P$ pixels and your stride is $S$, the resulting image will have width/height:\n",
    "\n",
    "$$ W_1 = \\frac{W_0 - F + 2P}{S} + 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.2**: You input an image of dimensions $28 \\times 28 \\times 3$, use a padding of 2, a stride of 1,\n",
    "and then slide your $5 \\times 5 \\times 3$ filter across the image. What is the dimensionality of the resulting activation map?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION:\n",
    "\n",
    "Since the input image has width $W_0 = 28$, the filter has width $F=5$, the padding is $P=2$, and the stride is $S = 1$, the width of the resulting activation map is\n",
    "\\begin{align}\n",
    "W_1 &= \\frac{W_0 - F + 2P}{S} + 1 \\\\\n",
    "&= \\frac{28 - 5 + 2(2)}{1} + 1 \\\\\n",
    "&= 28.\n",
    "\\end{align}\n",
    "Since there was only one filter, the dimensionality of the resulting activation is $28 \\times 28 \\times 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.3**: Let's say you now want to use a stride of 2, instead of 1. What problem does this immediately cause?\n",
    "How can we solve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION:\n",
    "\n",
    "Using a stride of 2 instead of 1, we are left with a trailing fraction upon division since the numerator is $27$ and the denominator is $2$. To solve this problem, we usually increase the padding but since the padding is multiplied by $2$, the numerator will always remain an odd number and thus we will still be left with a remainder upon division. If we want to keep a stride of $2$, we must then change the dimensionality of the filter to an even-numbered width and height (ie $4 \\times 4 \\times 3$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you will use the MNIST dataset again. In the cell below I have written some code to prepare it somewhat. For your own sake, try to understand what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T20:12:46.465418Z",
     "start_time": "2019-09-30T20:12:46.027204Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape data so it has a channel dimension\n",
    "rows, cols = x_train.shape[-2:]\n",
    "x_train = x_train.reshape(x_train.shape[0], rows, cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], rows, cols, 1)\n",
    "\n",
    "# Convert pixel intensities to values between 0 and 1\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "    \n",
    "# Convert target vectors to one-hot encoding\n",
    "num_classes = len(set(y_train))\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.5**: Implement Nielsen's [last convolutional neural network](http://neuralnetworksanddeeplearning.com/chap6.html#convolutional_neural_networks_in_practice)\n",
    "(the one with two convolutional layers and dropout), and score an accuracy higher than 98%. It doesn't have to be\n",
    "fully identical, but his solution is pretty great, so getting close is a cheap way to score a high accuracy.\n",
    ">\n",
    "> *Hint:* [here](https://keras.io/examples/mnist_cnn/) is an example of a CNN in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION:\n",
    "\n",
    "Here we define a sequential model with two convolutional layers, each with 24 and 40 $5 \\times 5$ filters and a max pooling layer, respectively. The output tensor is flattened before the fully connected and dropout layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=1, activation='relu', input_shape=(28,28,1,)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=1, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "display_name": "Python 3",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
